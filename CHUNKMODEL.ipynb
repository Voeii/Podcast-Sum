{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from get_transcripts import semantic_segmentation, extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>best_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spotify:episode:4KRC1TZ28FavN3J5zLHEtQ</td>\n",
       "      <td>What's up fellas? So I got a patron supported...</td>\n",
       "      <td>All right guys now as y'all guys might know so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spotify:episode:4tdDQcsBOUVWnA9XrpgTzS</td>\n",
       "      <td>If you are bored you are boring.  One of my ki...</td>\n",
       "      <td>It was the first and last time I ever said tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spotify:episode:626YAxomH0HZ6nCW9NLlGY</td>\n",
       "      <td>Visit Larisa English club.com English everyday...</td>\n",
       "      <td>Prepositions of movement review two is the sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spotify:episode:6AUFl7KQWN6pzGFEIEKFQu</td>\n",
       "      <td>So so and salutations Summers and welcome to t...</td>\n",
       "      <td>It only seems fitting to walk you through a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spotify:episode:6IDbemwG5t6XMlctbqcna7</td>\n",
       "      <td>Hi everyone. This is Justin from a liquidy pla...</td>\n",
       "      <td>This week on Nothing But A Bob Thang, Nathan a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               episode id  \\\n",
       "0  spotify:episode:4KRC1TZ28FavN3J5zLHEtQ   \n",
       "1  spotify:episode:4tdDQcsBOUVWnA9XrpgTzS   \n",
       "2  spotify:episode:626YAxomH0HZ6nCW9NLlGY   \n",
       "3  spotify:episode:6AUFl7KQWN6pzGFEIEKFQu   \n",
       "4  spotify:episode:6IDbemwG5t6XMlctbqcna7   \n",
       "\n",
       "                                          transcript  \\\n",
       "0   What's up fellas? So I got a patron supported...   \n",
       "1  If you are bored you are boring.  One of my ki...   \n",
       "2  Visit Larisa English club.com English everyday...   \n",
       "3  So so and salutations Summers and welcome to t...   \n",
       "4  Hi everyone. This is Justin from a liquidy pla...   \n",
       "\n",
       "                                        best_summary  \n",
       "0  All right guys now as y'all guys might know so...  \n",
       "1  It was the first and last time I ever said tha...  \n",
       "2  Prepositions of movement review two is the sec...  \n",
       "3  It only seems fitting to walk you through a fe...  \n",
       "4  This week on Nothing But A Bob Thang, Nathan a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"gold_set_cleaned.tsv\", sep='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isChunkUseful(chunk, summary, metric, threshold, verbose=False):\n",
    "    score = metric(chunk, summary)\n",
    "    if verbose: print(f\"\\tChunck: {chunk}\\n\\tSummary: {summary}\\n\\tScore: {score}\")\n",
    "\n",
    "    if score < threshold:\n",
    "        result = False\n",
    "    else:\n",
    "        result = True\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(candidate, reference, type='rouge-l', metric='f'):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0][type][metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features and targets: 100%|██████████| 141/141 [20:05<00:00,  8.55s/it]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.20\n",
    "metric = rouge_score\n",
    "verbose = False\n",
    "\n",
    "features = []\n",
    "targets = []\n",
    "\n",
    "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for i in tqdm(range(len(dataset)), desc=\"Extracting features and targets\"):\n",
    "    if verbose: print(f\"Episode: {i}\")\n",
    "    chunks = semantic_segmentation(dataset.transcript[i], sentence_encoder)\n",
    "    description = dataset.best_summary[i]\n",
    "\n",
    "    num_chunks = len(chunks)\n",
    "    if verbose: print(f\"Num chunks: {num_chunks}\")\n",
    "\n",
    "    for j in range(num_chunks):\n",
    "        if verbose: print(f\"\\tChunk {j}\")\n",
    "        features.append(extract_features(chunks[j], sentence_encoder))\n",
    "        if isChunkUseful(' '.join(chunks[j]), description, metric, threshold, verbose):\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "\n",
    "y = np.array(targets)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of useful chunks: 9.119270458363331%\n",
      "Percentage of unuseful chunks: 90.88072954163667%\n"
     ]
    }
   ],
   "source": [
    "positive = y[y==1].shape[0]\n",
    "negative = y.shape[0] - positive\n",
    "print(f\"Percentage of useful chunks: {positive/(positive+negative)*100}%\")\n",
    "print(f\"Percentage of unuseful chunks: {negative/(positive+negative)*100}%\")\n",
    "\n",
    "chunck_classification_dataset = np.hstack((X, y))\n",
    "df_chunk = pd.DataFrame(chunck_classification_dataset)\n",
    "df_chunk.to_csv(\"chunk_classification_dataset.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunck_classification_dataset = pd.read_csv(\"chunk_classification_dataset.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chunck_classification_dataset.iloc[:,-1]\n",
    "X = chunck_classification_dataset.drop(chunck_classification_dataset.columns[[-1]], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_positive = X_train[y_train>0]\n",
    "X_train_negative = X_train[y_train==0][:X_train_positive.shape[0]]\n",
    "y_train_positive = y_train[y_train>0]\n",
    "y_train_negative = y_train[y_train==0][:X_train_positive.shape[0]]\n",
    "\n",
    "X_train = np.vstack((X_train_positive,X_train_negative))\n",
    "y_train = np.hstack((y_train_positive, y_train_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.6683 - precision_1: 0.5250 - recall_1: 0.5662 - val_loss: 1.3647 - val_precision_1: 0.0947 - val_recall_1: 0.9961\n",
      "Epoch 2/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0977 - precision_1: 0.5641 - recall_1: 0.8122 - val_loss: 0.6237 - val_precision_1: 0.2009 - val_recall_1: 0.4942\n",
      "Epoch 3/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6681 - precision_1: 0.6627 - recall_1: 0.6458 - val_loss: 0.6804 - val_precision_1: 0.1751 - val_recall_1: 0.6718\n",
      "Epoch 4/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6128 - precision_1: 0.7206 - recall_1: 0.7885 - val_loss: 0.6540 - val_precision_1: 0.1860 - val_recall_1: 0.6680\n",
      "Epoch 5/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5261 - precision_1: 0.7883 - recall_1: 0.7868 - val_loss: 0.6842 - val_precision_1: 0.1766 - val_recall_1: 0.7529\n",
      "Epoch 6/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.4541 - precision_1: 0.8212 - recall_1: 0.8451 - val_loss: 0.8002 - val_precision_1: 0.1659 - val_recall_1: 0.7104\n",
      "Epoch 7/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3480 - precision_1: 0.9015 - recall_1: 0.8637 - val_loss: 0.6831 - val_precision_1: 0.1887 - val_recall_1: 0.6293\n",
      "Epoch 8/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2773 - precision_1: 0.9406 - recall_1: 0.9297 - val_loss: 0.7688 - val_precision_1: 0.2024 - val_recall_1: 0.6564\n",
      "Epoch 9/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2037 - precision_1: 0.9563 - recall_1: 0.9688 - val_loss: 1.4911 - val_precision_1: 0.1454 - val_recall_1: 0.8108\n",
      "Epoch 10/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1554 - precision_1: 0.9915 - recall_1: 0.9880 - val_loss: 1.3913 - val_precision_1: 0.1554 - val_recall_1: 0.7954\n",
      "Epoch 11/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1224 - precision_1: 0.9992 - recall_1: 1.0000 - val_loss: 1.2730 - val_precision_1: 0.1662 - val_recall_1: 0.7066\n",
      "Epoch 12/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1048 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 1.2623 - val_precision_1: 0.1650 - val_recall_1: 0.6911\n",
      "Epoch 13/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0988 - precision_1: 1.0000 - recall_1: 0.9950 - val_loss: 1.2207 - val_precision_1: 0.1736 - val_recall_1: 0.6950\n",
      "Epoch 14/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0919 - precision_1: 1.0000 - recall_1: 0.9997 - val_loss: 1.3052 - val_precision_1: 0.1647 - val_recall_1: 0.7181\n",
      "Epoch 15/15\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0886 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 1.5857 - val_precision_1: 0.1513 - val_recall_1: 0.7645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.573973100690658\n",
      "Precision: [0.95769764 0.1512605 ]\n",
      "Recall: [0.55417335 0.76447876]\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(384,))\n",
    "x = keras.layers.Dense(512, activation='relu')(inputs)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "x = keras.layers.Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "x = keras.layers.Dense(128, activation='relu', kernel_regularizer='l2')(x)\n",
    "output = keras.layers.Dense(1, activation='sigmoid', kernel_regularizer='l2')(x)\n",
    "model = keras.Model(inputs, output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=15,\n",
    "    validation_split=0.15,\n",
    "    validation_data=(X_test,y_test),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n",
    ")\n",
    "\n",
    "model.save(\"modelChunkNN.h5\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if y>0.5 else 0 for y in y_pred]\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average=None)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average=None)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
